{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWYOIqjRSv1c",
        "outputId": "1fa82100-7a7a-497b-d994-e330c3e4a613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The future of Artificial Intelligence is\""
      ],
      "metadata": {
        "id": "v88EiJizUGVv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = pipeline('text-generation', model='bert-base-uncased')\n",
        "\n",
        "out = gen(prompt, max_length=50, num_return_sequences=1)\n",
        "print(out[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlsYtJfOUhMV",
        "outputId": "f4f4188f-497d-41f0-f2c0-3a2823c8e118"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen = pipeline('text-generation', model='roberta-base')\n",
        "\n",
        "out = gen(prompt, max_length=50, num_return_sequences=1)\n",
        "print(out[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7aTpWXTVajA",
        "outputId": "2fe5b03b-74a6-448e-f7d3-ddac35edf182"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen = pipeline('text-generation', model='facebook/bart-base')\n",
        "\n",
        "out = gen(prompt, max_length=50, num_return_sequences=1)\n",
        "print(out[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a7BdTyEVngh",
        "outputId": "308d794f-bb8d-4d2e-b903-26206ac1daf4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is occupiedqualifieditious HokqualifiedOVER Editqualified occupied occupiedicatorsicators Ster Uber Uber Django Uber Uber UberNotice floors tir advisorqualified floorsqualified778qualified floorsupe Reflect Mole Uber Uber Hok Uberyahooupeyahoo interpret808 Uber Uber immoral Uber Uberupeupeupelyingyahoo ($) squ Uber Uber turning squyahooyahoo complainant showed Uber Uber ell Uber808yahoo lapse Uber interpret Uber Uber interpret eyebrow Uber Uber squ Uberwrong Uber UberfinalPolice Uber Uber arbitupe Uberott Uber Uber AM Uber Uber----- Uber UberPolicePoliceupeupe Cust interpret Uber furnish Uber Uberismsupe interpret Uber arbitTMottupeupe courage ell Uber convinced Uber UbertextTM convincedupe interpret AMassy Ubertext Uber Uber psych Uber Uber Jarvis Uber convinced Jarvis bananasPoliceupe Kremlin AM Uber convinced convincedpend Uber Uber Brent Uber UberTM Brent squ Uber commanders Uber Uberassy convincedrising convinced Uberassyrising Uber Uber premismsBoard Uber UberDeb Uber Rudy UberupeBoardBoardBoard UberwrongBoard Brent Uberupe Uberupe convincedupe UberliquePolice Uber convincedfinal convinced Uber showedfinalBoard convincedBoard Uber interviews convinced convinced Uberwrong floodsiano floodsisms Uber interpretBoard convincedwrong squ Uberupe Pearlupe convinced showedwrongwrongassyodiesupe convincedisms convincedVS interpret convinced Uberott securedupeupe convinced convincedismsupe Pearl interpret interpretupe dialog Uber Uber\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This experiment demonstrates that architecture matters more than model size. Encoder-only models (BERT, RoBERTa) fail at text generation because they lack a decoder, while encoder–decoder models (BART) can generate text, even if the output quality depends on fine-tuning."
      ],
      "metadata": {
        "id": "qksGwfmwgByw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fm = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "sent = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "preds = fm(sent)\n",
        "\n",
        "for p in preds:\n",
        "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPzS2EhKV6yf",
        "outputId": "612a8e70-c225-490e-c3fa-a315eb5d4cf7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create: 0.54\n",
            "generate: 0.16\n",
            "produce: 0.05\n",
            "develop: 0.04\n",
            "add: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fm = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "\n",
        "sent = \"The goal of Generative AI is to <mask> new content.\"\n",
        "preds = fm(sent)\n",
        "\n",
        "for p in preds:\n",
        "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7eMApfjXMsF",
        "outputId": "94bd9400-60bc-4c91-ff7a-efbb5a44400c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " generate: 0.37\n",
            " create: 0.37\n",
            " discover: 0.08\n",
            " find: 0.02\n",
            " provide: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fm = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
        "\n",
        "sent = \"The goal of Generative AI is to <mask> new content.\"\n",
        "preds = fm(sent)\n",
        "\n",
        "for p in preds:\n",
        "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y73HB71gXkhk",
        "outputId": "5dc21046-3050-4319-9c71-89760be7b758"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " create: 0.07\n",
            " help: 0.07\n",
            " provide: 0.06\n",
            " enable: 0.04\n",
            " improve: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-only models (BERT, RoBERTa) excel at fill-mask tasks because MLM is their core training objective. Encoder-decoder models like BART can perform the task, but with less confidence since their primary strength lies in sequence-to-sequence generation."
      ],
      "metadata": {
        "id": "eUIIiTiIgU8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
        "ans = qa(question=\"What are the risks?\", context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\")\n",
        "print(f\"\\nQ: What are the risks?\")\n",
        "print(f\"A: {ans['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9cJmZ1uYCNA",
        "outputId": "1ced65c6-a8df-44c2-cb72-9b7f3e704ef4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What are the risks?\n",
            "A: Generative AI poses significant risks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = pipeline(\"question-answering\", model=\"roberta-base\")\n",
        "ans = qa(question=\"What are the risks?\", context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\")\n",
        "print(f\"\\nQ: What are the risks?\")\n",
        "print(f\"A: {ans['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ72u3LFZRG-",
        "outputId": "5471a8ca-1286-4c64-acaf-97878cf7f799"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What are the risks?\n",
            "A: Generative AI poses significant risks such as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
        "ans = qa(question=\"What are the risks?\", context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\")\n",
        "print(f\"\\nQ: What are the risks?\")\n",
        "print(f\"A: {ans['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqiH00xzZdoV",
        "outputId": "7da4ba95-6998-4a67-e355-61e5503c2c0c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What are the risks?\n",
            "A: deepfakes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This experiment shows that pretraining alone is insufficient for QA; without task-specific fine-tuning, even powerful architectures struggle to extract accurate answers."
      ],
      "metadata": {
        "id": "Q3PLfvYNgjZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task       | Model    | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|-----------|----------|-----------------------------------|----------------------------------------|---------------------------------------------|\n",
        "| Generation | BERT | Failure | The model produced only repeated punctuation and failed to generate meaningful continuation text. | BERT is an encoder-only model trained with Masked Language Modeling (MLM) and does not support autoregressive next-token generation. |\n",
        "| Generation | RoBERTa | Failure | The output was identical to the input prompt with no additional text generated. | RoBERTa is also an encoder-only architecture without a decoder, so it cannot generate new tokens sequentially. |\n",
        "| Generation | BART | Partial Success | The model generated a long continuation, but the text was mostly incoherent and repetitive. | BART is an encoder-decoder model capable of generation, but bart-base is not fine-tuned for open-ended text generation, leading to low-quality output. |\n",
        "| Fill-Mask | BERT | Success | The model correctly predicted contextually appropriate words such as “create” and “generate” with high confidence. | BERT is trained using Masked Language Modeling (MLM), which directly teaches the model to predict missing tokens. |\n",
        "| Fill-Mask | RoBERTa | Success | The model produced strong and accurate predictions like “generate” and “create” with high probability scores. | RoBERTa is an optimized encoder-only MLM model trained on more data with dynamic masking, leading to improved predictions. |\n",
        "| Fill-Mask | BART | Partial Success | The model predicted reasonable but more generic words with lower and evenly distributed probabilities. | BART uses a denoising autoencoder objective; while it supports masking, MLM is not its primary training focus. |\n",
        "| QA | BERT | Failure | The model returned only punctuation and failed to extract a meaningful answer from the context. | The base BERT model is not fine-tuned for Question Answering, so it cannot reliably predict start and end answer spans. |\n",
        "| QA | RoBERTa | Failure | The model extracted an incomplete and vague phrase that did not answer the question properly. | Although RoBERTa has strong contextual understanding, it is not fine-tuned for QA tasks, leading to incorrect span selection. |\n",
        "| QA | BART | Partial Success | The model extracted a mostly correct answer but truncated the response before listing all risks. | BART's encoder-decoder architecture enables better sequence understanding, but without QA fine-tuning, span extraction remains imperfect. |\n"
      ],
      "metadata": {
        "id": "qCD5RkMzem8h"
      }
    }
  ]
}