{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU3sN1bU5dBR",
        "outputId": "e2bde57e-7079-4482-f8c6-0988241cec54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, nltk"
      ],
      "metadata": {
        "id": "cYsxxc3N5x5g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"unit 1.txt\""
      ],
      "metadata": {
        "id": "YDdUBncz52sr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "  print(\"File loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: '{file_path}' not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5itSz_V585B",
        "outputId": "e734b527-ed5e-44e9-ef05-0ba76fe913e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Data Preview ---\")\n",
        "print(text[:500] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v2kbUzV6AzD",
        "outputId": "ae665ada-27be-4413-b48a-15d50aba0295"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Data Preview ---\n",
            "Generative AI and Its Applications: A Foundational Briefing\n",
            "\n",
            "Executive Summary\n",
            "\n",
            "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(20)"
      ],
      "metadata": {
        "id": "NuCM-cCN6Dqo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I chose Generative AI as my elective because\""
      ],
      "metadata": {
        "id": "FATwc2nJ6Gie"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the pipeline with the specific model and load pretrained model\n",
        "fast_generator = pipeline('text-generation', model='distilgpt2')\n",
        "\n",
        "# Generate text\n",
        "# num_return_sequences=3- It will generate 3 different continuations\n",
        "output_fast = fast_generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_fast[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fPuehJv6JCH",
        "outputId": "d1f402fe-6f0d-47b5-fb39-a9f79dd92b50"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I chose Generative AI as my elective because of the huge amount of work and work that has been done by the AI community. However, my goal has been to improve the AI community as a whole. There are many things that can be improved at the same time and this is why I decided to invest in it.\n",
            "\n",
            "\n",
            "I wanted to make one more point and give my thoughts to those who are interested in making a contribution to my project but I think that there are a lot of potential and this is a good way to start. I want to create a huge group of people who know what the project needs to be and I want to make sure this is something that is also in the best interest of all.\n",
            "I want to thank everyone who is involved in the project and to everyone who has been involved with it. I want to thank everyone who has contributed to the project and to everyone who has been involved with it.\n",
            "The Future of AI has been made possible by the contributions to the AI community and I want to thank everyone who has contributed to the project and to everyone who has contributed to the project and to everyone who has contributed to the project and to everyone who has contributed to the project and to everyone who has contributed to the project and to everyone who has contributed to the project and to everyone who\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smart_generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "output_smart = smart_generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output_smart[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLhw4Bah6S8w",
        "outputId": "e8eaf02b-ed00-4af0-a6bf-b86c9f2f3bfe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I chose Generative AI as my elective because it is not just a good idea. It is an elective where you will elect people who have the skill set to manage a network of smart machines. I believe that the best way to make AI better is to enable them to operate through a smart contract, as well as a smart contract that makes decisions for them based on the inputs and outputs of the smart contract.\n",
            "\n",
            "The best way to do that is to create a smart contract that allows you to control the network of smart machines. The idea is to create a smart contract that allows you to make decisions based on the inputs and outputs of the smart contract.\n",
            "\n",
            "The best way to do that is to create a smart contract that allows you to control the network of smart machines. The idea is to create a smart contract that allows you to make decisions based on the inputs and outputs of the smart contract.\n",
            "\n",
            "The simplest thing to do is to create a smart contract that allows you to control the network of smart machines. The idea is to create a smart contract that allows you to make decisions based on the inputs and outputs of the smart contract.\n",
            "\n",
            "The simplest thing to do is to create a smart contract that allows you to control the network of smart machines. The idea is to create a smart contract that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Initialize the Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "04nA2Y9A6fwS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"Learning Tokenization under NLP Fundamentals\""
      ],
      "metadata": {
        "id": "XuymsIuH6idf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(sample_sentence)\n",
        "print(f\"Tokens: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehCzTGhE6lb4",
        "outputId": "331a03af-17d1-4291-c277-5efdf71e12ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Learning', 'ĠToken', 'ization', 'Ġunder', 'ĠN', 'LP', 'ĠFund', 'ament', 'als']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"Token IDs: {token_ids}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARE08AHQ6oJn",
        "outputId": "56d95f53-27ba-481d-dd0d-13e7231d5498"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [41730, 29130, 1634, 739, 399, 19930, 7557, 3263, 874]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data-used for POS\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "# Sentence splitting & word tokenization\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "nltk.download('punkt_tab', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43vu34wt6qz2",
        "outputId": "4051bad3-b146-4ad8-f0af-075da528cee6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(nltk.word_tokenize(sample_sentence))\n",
        "print(f\"POS Tags: {pos_tags}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncBkzr-G6wKJ",
        "outputId": "d7a648e8-c32f-484c-9c69-f6fd60cbe187"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('Learning', 'VBG'), ('Tokenization', 'NNP'), ('under', 'IN'), ('NLP', 'NNP'), ('Fundamentals', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize NER Pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdJ88xWG6z4l",
        "outputId": "03500057-b87d-4596-f4ed-30d9a605bc2c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snippet = text[:500]\n",
        "entities = ner_pipeline(snippet)\n",
        "#allignment\n",
        "print(f\"{'Entity': <20} | {'Type':<10} | {'Score':<5}\")\n",
        "print(\"-\"*45)\n",
        "for entity in entities:\n",
        "  if entity['score'] > 0.90: #print scores only if greater than 90\n",
        "    print(f\"{entity['word']:<20} | {entity['entity_group']:<10} | {entity['score']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07SPztkQ68B7",
        "outputId": "89201fd1-dd48-49c0-e14c-d67d07c4c081"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity               | Type       | Score\n",
            "---------------------------------------------\n",
            "AI                   | MISC       | 0.98\n",
            "PES University       | ORG        | 0.99\n",
            "AI                   | MISC       | 0.98\n",
            "Language Models      | MISC       | 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's extract a specific section for summarization\n",
        "transformer_section = \"\"\" Building an LLM is a two-stage process involving pretraining on vast, diverse datasets to build a general understanding of language, followed by finetuning to specialize the model for specific tasks. The functionality of these models relies on core principles of Natural Language Processing (NLP), such as converting text into numerical vectors via word embeddings and understanding grammatical structure through Part-of-Speech (POS) tagging and Named Entity Recognition (NER).\"\"\""
      ],
      "metadata": {
        "id": "IsUqF0Cj6_X_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fast_sum = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "res_fast = fast_sum(transformer_section, max_length=50, min_length=25, do_sample=False)\n",
        "print(res_fast[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjuW0ihj7B5V",
        "outputId": "2b5b2282-fa31-4383-b573-fe4da7924275"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Building an LLM is a two-stage process involving pretraining on vast, diverse datasets to build a general understanding of language . The functionality of these models relies on core principles of Natural Language Processing (NLP), such as converting text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smart_sum = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "res_smart = smart_sum(transformer_section, max_length=80, min_length=40, do_sample=False)\n",
        "print(res_smart[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJaYGEmb7NNG",
        "outputId": "e7eaf489-7483-46cd-a56e-4d673872dd57"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building an LLM is a two-stage process involving pretraining on vast, diverse datasets. The functionality of these models relies on core principles of Natural Language Processing (NLP), such as converting text into numerical vectors via word embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INt51Hx07fMu",
        "outputId": "0c527dc3-fd56-4eaf-eeda-53528d7b2beb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is the fundamental innovation of the Transformer?\",\n",
        "    \"What are the risks of using Generative AI?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    res = qa_pipeline(question=q, context=text[:5000])\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrglq13F7lHG",
        "outputId": "f99c5ce2-b238-419b-fb8c-c5238187de48"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is the fundamental innovation of the Transformer?\n",
            "A: to identify hidden patterns, structures, and relationships within the data\n",
            "\n",
            "Q: What are the risks of using Generative AI?\n",
            "A: data privacy, intellectual property, and academic integrity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFUsM5-87qJF",
        "outputId": "a8ab5046-262f-4a1a-f49b-99ab7ca646a7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_sentence = \"The goal of Generative AI is to create new [MASK].\"\n",
        "preds = mask_filler(masked_sentence)\n",
        "\n",
        "for p in preds:\n",
        "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
      ],
      "metadata": {
        "id": "AC9JJBIV7yAI",
        "outputId": "e6c43efb-4122-452d-ee87-ef912428d26b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "applications: 0.06\n",
            "ideas: 0.05\n",
            "problems: 0.05\n",
            "systems: 0.04\n",
            "information: 0.03\n"
          ]
        }
      ]
    }
  ]
}